{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the CrossEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentence_transformers\n",
    "from importlib import reload\n",
    "\n",
    "reload(sentence_transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from sentence_transformers import LoggingHandler, util\n",
    "from tqdm.autonotebook import tqdm, trange\n",
    "\n",
    "#### Just some code to print debug information to stdout\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\", level=logging.INFO, handlers=[LoggingHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "#### /print debug information to stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "##Dataset.remove_columns will prune columns better!!\n",
    "\n",
    "datafiles = {'train':'training_dataset.jsonl', 'validation': 'eval_dataset.jsonl', 'test':'test_dataset.jsonl'}\n",
    "\n",
    "dataset = load_dataset(\"json\",\n",
    "                       data_files=datafiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.readers import InputExample\n",
    "\n",
    "train_samples = []\n",
    "dev_samples = []\n",
    "test_samples = []\n",
    "\n",
    "for row in dataset['train']:\n",
    "    train_samples.append(InputExample(texts=[row['text1'], row['text2']], label=row['label']))\n",
    "\n",
    "for row in dataset['validation']:\n",
    "    dev_samples.append(InputExample(texts=[row['text1'], row['text2']], label=row['label']))\n",
    "\n",
    "for row in dataset['test']:\n",
    "    test_samples.append(InputExample(texts=[row['text1'], row['text2']], label=row['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(train_samples, shuffle=True, batch_size= train_batch_size)\n",
    "dev_dataloader = DataLoader(dev_samples, shuffle=True, batch_size= train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from datetime import datetime\n",
    "\n",
    "model_save_path = \"output/training_allnli-\" + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Define our CrossEncoder model. We use distilroberta-base as basis and setup it up to predict 3 labels\n",
    "model = CrossEncoder(\"distilroberta-base\",\n",
    "                     num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentence_transformers.cross_encoder\n",
    "from sentence_transformers.cross_encoder.evaluation import CEBinaryClassificationEvaluator, CEBinaryAccuracyEvaluator, CEF1Evaluator\n",
    "from sentence_transformers.evaluation import SequentialEvaluator\n",
    "import math\n",
    "\n",
    "import sentence_transformers.evaluation\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "# During training, we use various evaluators to measure the performance on the dev set\n",
    "binary_clf_eval = CEBinaryClassificationEvaluator.from_input_examples(dev_samples)\n",
    "binary_acc_eval = CEBinaryAccuracyEvaluator.from_input_examples(dev_samples)\n",
    "binary_f1_eval = sentence_transformers.cross_encoder.evaluation.CEF1Evaluator.from_input_examples(dev_samples)\n",
    "\n",
    "evaluator = SequentialEvaluator([binary_clf_eval, binary_acc_eval])\n",
    "\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1)  # 10% of train data for warm-up\n",
    "logger.info(\"Warmup-steps: {}\".format(warmup_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## There is a bug in the library - It won't be fixed until the major overhaul of CrossEncoder is completed\n",
    "# https://github.com/UKPLab/sentence-transformers/issues/2737\n",
    "\n",
    "##Â As a temp fix I have hashed out lines 495-498 in 'sentence-transformers/cross-encoder/CrossEncoder.py'\n",
    "\n",
    "## Indentified a second bug where Argmax is called on alist with axis=1 which causes an error\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_dataloader=train_dataloader,\n",
    "    epochs=num_epochs,\n",
    "    evaluation_steps=500,\n",
    "    warmup_steps=warmup_steps,\n",
    "    evaluator=evaluator,\n",
    "    save_best_model=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('compat_matrix_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CrossEncoder('compat_matrix_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_prediction(row_number):\n",
    "    pred = model.predict([dataset['test'][row_number]['text1'],\n",
    "                          dataset['test'][row_number]['text2']])\n",
    "    result = dataset['test'][row_number]['label']\n",
    "    return pred, result\n",
    "\n",
    "y_pred = []\n",
    "y = []\n",
    "for i in range(0,len(dataset['test'])):\n",
    "    pred, result = validate_prediction(i)\n",
    "    y_pred.append(pred)\n",
    "    y.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised_ypred = []\n",
    "for value in y_pred:\n",
    "\n",
    "    if value < 0.5:\n",
    "        normalised_ypred.append(0)\n",
    "    elif value > 0.5:\n",
    "        normalised_ypred.append(1)\n",
    "    else:\n",
    "        print(f'{value}: appending -1')\n",
    "        normalised_ypred.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.Series(normalised_ypred).unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y, normalised_ypred, labels=[0,1])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=[0,1])\n",
    "\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
