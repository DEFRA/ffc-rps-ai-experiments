{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Compatibility Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "def load_config(yaml_config: str):\n",
    "    with open(yaml_config) as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    return config\n",
    "\n",
    "config = load_config('./config.yaml')['data_processing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compat = pandas.read_excel(Path(config['input_compat_data_folder']) / config['input_compat_excel_filename'])\n",
    "df_compat = df_compat[['OPTION CODE', 'OPTION CODE COMPAT']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Action Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to replace u/XXXX with the corresponding Unicode character\n",
    "def replace_unicode_notation(match):\n",
    "    unicode_code = match.group(1)\n",
    "    return chr(int(unicode_code, 16))\n",
    "\n",
    "sfi_action_files =  f\"{config['sfi_actions_folder']}*.txt\"\n",
    "glob.glob(sfi_action_files)\n",
    "action_json = {}\n",
    "\n",
    "for filepath in glob.glob(sfi_action_files):\n",
    "    code = filepath.split('/')[-1].split('-')[0].upper()\n",
    "    with open(filepath, 'r') as f:\n",
    "        text = f.read()\n",
    "    cleaned_text = re.sub(r'u/([0-9A-Fa-f]{4})', replace_unicode_notation, text)\n",
    "    action_json[code] = cleaned_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_action_list = [i for i in action_json]\n",
    "random.shuffle(full_action_list)\n",
    "\n",
    "# full_action_list = [action_json.keys()]\n",
    "\n",
    "\n",
    "train_actionlist = full_action_list[:86] #86 actions to use for training\n",
    "eval_actionlist = full_action_list[86:91] # 5 actions for eval during training\n",
    "test_actionlist = full_action_list[:91] #removing 10 actions to serve as testing which will not be seen by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.54054054054054"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_compat.groupby('OPTION CODE').count().reset_index(drop=False)['OPTION CODE COMPAT'].mean()#.sort_values(by='OPTION CODE COMPAT', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_training_dataset.jsonl has been deleted.\n",
      "training_dataset.jsonl has been deleted.\n",
      "full_eval_dataset.jsonl has been deleted.\n",
      "eval_dataset.jsonl has been deleted.\n",
      "full_test_dataset.jsonl has been deleted.\n",
      "test_dataset.jsonl has been deleted.\n"
     ]
    }
   ],
   "source": [
    "def create_cartesian(list1, list2):\n",
    "    product =  itertools.product(*[list1, list2])\n",
    "    return product\n",
    "\n",
    "\n",
    "def construct_dataset(cartesian, action_json, df_compat):\n",
    "    dataset = []\n",
    "    for i in cartesian:\n",
    "        try:\n",
    "            if i[0] == i[1]: #remove self references\n",
    "                continue\n",
    "            text1 = action_json[i[0]]\n",
    "            text2 = action_json[i[1]]\n",
    "            compatibility = len(df_compat.loc[(df_compat['OPTION CODE'] == i[0]) \\\n",
    "                                              & (df_compat['OPTION CODE COMPAT'] == i[1])])\n",
    "            entry =  {'text1': text1, 'text2': text2, 'code1': i[0], 'code2': i[1], 'label': compatibility}\n",
    "            dataset.append(entry)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def delete_file_if_exists(filename):\n",
    "    if os.path.exists(filename):\n",
    "        # Delete the file\n",
    "        os.remove(filename)\n",
    "        print(f\"{filename} has been deleted.\")\n",
    "    else:\n",
    "        print(f\"{filename} does not exist.\")\n",
    "\n",
    "\n",
    "# Todo: I have removed the labels from the data so that it doesn't make its way into the model for training. \n",
    "# I do remember the DataLoader class which we have to use having an argument for elements which are not used for training\n",
    "# If we can implement that then we only need to construct one dataset.\n",
    "def save_outputs(dataset, full_filename, reduced_filename):\n",
    "    delete_file_if_exists(full_filename)\n",
    "    delete_file_if_exists(reduced_filename)\n",
    "\n",
    "    for line in dataset:\n",
    "        with open(full_filename, 'a') as f:\n",
    "            f.write(json.dumps(line) + \"\\n\")\n",
    "        reduced_line = {'text1': line['text1'], 'text2': line['text2'], 'label': line['label']}\n",
    "        with open(reduced_filename, 'a') as f:\n",
    "            f.write(json.dumps(reduced_line) + \"\\n\")\n",
    "\n",
    "    return f'{full_filename} and {reduced_filename} datasets created'\n",
    "\n",
    "\n",
    "def pipeline(list1, list2, action_json, df_compat, full_filename, reduced_filename):\n",
    "    cartesian = create_cartesian(list1, list2)\n",
    "    dataset = construct_dataset(cartesian, action_json, df_compat)\n",
    "    save_outputs(dataset, full_filename, reduced_filename)\n",
    "\n",
    "\n",
    "pipeline(train_actionlist, train_actionlist, action_json, df_compat,'full_training_dataset.jsonl', 'training_dataset.jsonl')\n",
    "pipeline(eval_actionlist, eval_actionlist, action_json, df_compat,'full_eval_dataset.jsonl', 'eval_dataset.jsonl')\n",
    "pipeline(test_actionlist, full_action_list, action_json, df_compat,'full_test_dataset.jsonl', 'test_dataset.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
