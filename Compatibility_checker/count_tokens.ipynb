{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tiktoken\n",
    "\n",
    "def count_tokens_in_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    encoding = tiktoken.encoding_for_model(\"text-embedding-ada-002\")  # You can change the model name if needed\n",
    "    tokens = encoding.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "def main(folder_path):\n",
    "    results = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            num_tokens = count_tokens_in_file(file_path)\n",
    "            results.append(num_tokens)\n",
    "            # print(f\"File: {filename}, Tokens: {num_tokens}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Replace 'path_to_your_folder' with the actual path to the folder containing your text files\n",
    "folder_path = './actions'\n",
    "token_counts = main(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On average a document contains 568.2079207920792 tokens\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "mean_action = np.mean(token_counts)\n",
    "print(f'On average a document contains {mean_action} tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost for Embedding SFI24 for Vector Database is $0.00114778\n",
      "\n",
      "Cost to input 1 document to LLM is $0.0002841039603960396\n",
      "Assuming top-3 docs are sent, cost per input is now $0.0008523118811881189\n"
     ]
    }
   ],
   "source": [
    "cost_gpt3_input_per_token = 0.5 / 1_000_000\n",
    "cost_gpt3_output_per_token = 1.5 / 1_000_000\n",
    "cost_embedding_model = 0.02 / 1_000_000\n",
    "\n",
    "print(f\"Cost for Embedding SFI24 for Vector Database is ${np.sum(token_counts) * cost_embedding_model}\")\n",
    "print()\n",
    "print(f\"Cost to input 1 document to LLM is ${mean_action * cost_gpt3_input_per_token}\")\n",
    "print(f\"Assuming top-3 docs are sent, cost per input is now ${(mean_action * cost_gpt3_input_per_token) * 3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Detailed breakdown of a query as a repsonse is approx 700 tokens\n",
      "Cost for initial response is approximately $0.00105\n",
      "\n",
      "The Cost for to serve for a query answer pair is $0.0019023118811881188\n"
     ]
    }
   ],
   "source": [
    "print('A Detailed breakdown of a query as a repsonse is approx 700 tokens')\n",
    "cost_initial_response = cost_gpt3_output_per_token * 700\n",
    "\n",
    "print(f'Cost for initial response is approximately ${cost_initial_response}')\n",
    "print()\n",
    "\n",
    "cost_to_serve_1query =((mean_action * cost_gpt3_input_per_token) * 3) + cost_initial_response\n",
    "\n",
    "print(f'The Cost for to serve for a query answer pair is ${cost_to_serve_1query}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you require a follow-up question, You have two options:\n",
    "- Stateless Query where each question is answered independly (cost is as above, but not recommended)\n",
    "- Stateful Query where chat history is sent to the LLM (more useful)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If 1 followup query needed\n",
      "Second round input now costs 0.0012023118811881187\n",
      "Second round output still costs 0.00105\n",
      "Cost to serve for 2 queries is $0.004154623762376237\n"
     ]
    }
   ],
   "source": [
    "print('If one additional followup query is needed, the previous answer gets sent as an input')\n",
    "\n",
    "second_round_input_tokens = (mean_action * 3) + 700\n",
    "\n",
    "print(f'Second round input now costs {second_round_input_tokens * cost_gpt3_input_per_token}')\n",
    "print(f'Second round output still costs $ {cost_initial_response}')\n",
    "\n",
    "cost_to_serve_2query = cost_to_serve_1query + (second_round_input_tokens * cost_gpt3_input_per_token) + cost_initial_response\n",
    "print(f'Cost to serve for 2 queries is ${cost_to_serve_2query}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are about 1,500 users of find a day who spend about 6mins on the site. Let's assume they all ask 2 queries to the chatbot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In OpenAI API costs alone, running the service costs $6.231935643564356 per day\n",
      "This equates to $2274.65650990099 a year (assuming no new users)\n"
     ]
    }
   ],
   "source": [
    "daily_cost_to_serve_2queries = cost_to_serve_2query * 1500\n",
    "print(f\"In OpenAI API costs alone, running the service costs ${daily_cost_to_serve_2queries} per day\")\n",
    "print(f'This equates to ${daily_cost_to_serve_2queries * 365} a year (assuming no new users)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing Semantic search on an embedded database will cost $0.00114778 if you need to re-embed the entire document set (unlikely)\n",
      "More likely, it will cost approx $0.00001136 to add or change a document\n"
     ]
    }
   ],
   "source": [
    "print(f'Doing Semantic search on an embedded database will cost ${np.sum(token_counts) * cost_embedding_model} if you need to re-embed the entire document set (unlikely)')\n",
    "\n",
    "print(f'More likely, it will cost approx ${mean_action * cost_embedding_model:.8f} to add or change a document')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carbon Footprint\n",
      "Lets assume ChatGPT costs 0.00002 kWh per token (got from chatGPT- need to fact check)\n",
      "Average US carbon intensity for electricty is 0.2 kg CO2e per kWh.\n",
      "Assuming our 2-response query which requires 5509.247524752475 tokens per user\n",
      "Expand that to our 547500 yearly users, equals 3016313019.80198 tokens per year\n",
      "\n",
      "Giving an energy usage of 60326.26039603961 kWh\n",
      "Giving Carbon Emmisions of 12065.252079207923kg of Carbon a Year\n"
     ]
    }
   ],
   "source": [
    "llm_energy_per_token = 0.00002\n",
    "electricity_carbon_intensity = 0.2 #(https://www.itpenergised.com/new-uk-grid-emissions-factors-2023/#:~:text=The%20generation%20emissions%20factor%20for,year%20increase%20of%207%25).)\n",
    "\n",
    "print('Carbon Footprint')\n",
    "print(f'Lets assume ChatGPT costs {llm_energy_per_token:.5f} kWh per token (got from chatGPT- need to fact check)')\n",
    "print(f'Average US carbon intensity for electricty is {electricity_carbon_intensity} kg CO2e per kWh.')\n",
    "\n",
    "tokens_per_user = (mean_action * 3) + 700 + (mean_action * 3) + 700 + 700\n",
    "print(f'Assuming our 2-response query which requires {tokens_per_user} tokens per user')\n",
    "\n",
    "\n",
    "users_per_year = 1500 * 365\n",
    "total_tokens_per_year = users_per_year * tokens_per_user\n",
    "print(f'Expand that to our {users_per_year} yearly users, equals {total_tokens_per_year} tokens per year')\n",
    "print()\n",
    "\n",
    "\n",
    "total_kwh = total_tokens_per_year * llm_energy_per_token\n",
    "print(f'Giving an energy usage of {total_kwh} kWh')\n",
    "print(f'Giving Carbon Emmisions of {total_kwh * electricity_carbon_intensity}kg of Carbon a Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alternative Carbon Footprint Calculation\n",
      "Carbon emission per query is 0.005kg\n",
      "Across the 547500 users (using 2 querues) that equates to 5475.0kg of Carbon per Year\n"
     ]
    }
   ],
   "source": [
    "print('Alternative Carbon Footprint Calculation')\n",
    "kg_carbon_per_query = 0.005 #this is the high end, but is probably conservative given we are doing RAG\n",
    "print(f'Carbon emission per query is {kg_carbon_per_query}kg')\n",
    "print(f'Across the {users_per_year} users (using 2 querues) that equates to {(users_per_year * 2) * kg_carbon_per_query}kg of Carbon per Year')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
